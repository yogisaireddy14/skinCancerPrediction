{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3376422,"sourceType":"datasetVersion","datasetId":2035877}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/yogisaireddy/skincancerprediction?scriptVersionId=201879290\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport keras\nfrom keras import layers\nfrom tensorflow import data as tf_data\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","metadata":{"execution":{"iopub.status.busy":"2024-09-22T15:36:15.512829Z","iopub.execute_input":"2024-09-22T15:36:15.513784Z","iopub.status.idle":"2024-09-22T15:36:27.061639Z","shell.execute_reply.started":"2024-09-22T15:36:15.513742Z","shell.execute_reply":"2024-09-22T15:36:27.060835Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"datagen = ImageDataGenerator(\n    rescale=1./255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T15:37:25.877166Z","iopub.execute_input":"2024-09-22T15:37:25.878266Z","iopub.status.idle":"2024-09-22T15:37:25.882517Z","shell.execute_reply.started":"2024-09-22T15:37:25.878226Z","shell.execute_reply":"2024-09-22T15:37:25.881567Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_size = (244, 224)\nbatch_size = 32\n\ntrain_ds = keras.utils.image_dataset_from_directory(\n    r\"/kaggle/input/melanoma-skin-cancer-dataset-of-10000-images/melanoma_cancer_dataset/train\",\n    seed=1337,\n    image_size=image_size,\n    batch_size=batch_size,\n)\nval_ds = keras.utils.image_dataset_from_directory(\n    r\"/kaggle/input/melanoma-skin-cancer-dataset-of-10000-images/melanoma_cancer_dataset/test\",\n    seed=1337,\n    image_size=image_size,\n    batch_size=batch_size,\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T15:37:29.41827Z","iopub.execute_input":"2024-09-22T15:37:29.419299Z","iopub.status.idle":"2024-09-22T15:37:41.27719Z","shell.execute_reply.started":"2024-09-22T15:37:29.419249Z","shell.execute_reply":"2024-09-22T15:37:41.27627Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nnum_classes = len(np.unique(train_ds))\nprint(\"Number of classes in the dataset:\", num_classes)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T10:59:20.994303Z","iopub.execute_input":"2024-09-22T10:59:20.995113Z","iopub.status.idle":"2024-09-22T10:59:21.000273Z","shell.execute_reply.started":"2024-09-22T10:59:20.995056Z","shell.execute_reply":"2024-09-22T10:59:20.999398Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\nval_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    r\"/kaggle/input/melanoma-skin-cancer-dataset-of-10000-images/melanoma_cancer_dataset/train\",  # Replace with your training data path\n    target_size=(224, 224),\n    batch_size=32,\n    # Use categorical for multi-class labels\n    class_mode='categorical'\n)\nvalidation_generator = val_datagen.flow_from_directory(\n    r\"/kaggle/input/melanoma-skin-cancer-dataset-of-10000-images/melanoma_cancer_dataset/test\",  # Replace with your validation data path\n    target_size=(224, 224),\n    batch_size=32,\n    # Use categorical for multi-class labels\n    class_mode='categorical'\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T15:37:57.787367Z","iopub.execute_input":"2024-09-22T15:37:57.788055Z","iopub.status.idle":"2024-09-22T15:37:59.507714Z","shell.execute_reply.started":"2024-09-22T15:37:57.788015Z","shell.execute_reply":"2024-09-22T15:37:59.506995Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import f1_score, recall_score, accuracy_score,precision_score\nfrom keras.metrics import MeanSquaredError\nfrom keras.losses import CategoricalCrossentropy # Changed to CategoricalCrossentropy\nfrom keras.utils import to_categorical\n\n\ndef evaluate_model(model, test_data, test_labels):\n    # Predict the probabilities for the test dataset\n    y_pred_prob = model.predict(test_data)\n\n    # Convert probabilities to class predictions\n    y_pred = np.argmax(y_pred_prob, axis=-1)\n\n    # One-hot encode test_labels\n    #test_labels = to_categorical(test_labels, num_classes=9) # This line is not needed as test_labels are already one-hot encoded.\n\n    # Calculate accuracy\n    accuracy = accuracy_score(np.argmax(test_labels, axis=-1), y_pred)\n\n    # Calculate F1 score\n    # Use 'weighted' average for multiclass data\n    f1 = f1_score(np.argmax(test_labels, axis=-1), y_pred, average='weighted')\n    precision = precision_score(np.argmax(test_labels, axis=-1), y_pred, average='weighted')\n\n    # Calculate recall\n    # Use 'weighted' average for multiclass data\n    recall = recall_score(np.argmax(test_labels, axis=-1), y_pred, average='weighted')\n\n    # Calculate categorical crossentropy loss\n    loss_fn = CategoricalCrossentropy() # Changed to CategoricalCrossentropy\n    loss = loss_fn(test_labels, y_pred_prob).numpy()\n\n    return loss, accuracy, f1, recall,precision","metadata":{"execution":{"iopub.status.busy":"2024-09-22T15:38:24.56945Z","iopub.execute_input":"2024-09-22T15:38:24.569874Z","iopub.status.idle":"2024-09-22T15:38:24.578928Z","shell.execute_reply.started":"2024-09-22T15:38:24.569827Z","shell.execute_reply":"2024-09-22T15:38:24.578093Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 10))\nfor images, labels in train_ds.take(1):\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        label_index = labels[i].numpy().argmax()\n        plt.title(label_index)  # Display the label index\n        plt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-22T10:59:28.061482Z","iopub.execute_input":"2024-09-22T10:59:28.061865Z","iopub.status.idle":"2024-09-22T10:59:29.394489Z","shell.execute_reply.started":"2024-09-22T10:59:28.061827Z","shell.execute_reply":"2024-09-22T10:59:29.39357Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_augmentation_layers = [\n    layers.RandomFlip(\"horizontal\"),\n    layers.RandomRotation(0.1),\n]\n\n\ndef data_augmentation(images):\n    for layer in data_augmentation_layers:\n        images = layer(images)\n    return images","metadata":{"execution":{"iopub.status.busy":"2024-09-22T11:00:05.457757Z","iopub.execute_input":"2024-09-22T11:00:05.458155Z","iopub.status.idle":"2024-09-22T11:00:05.469566Z","shell.execute_reply.started":"2024-09-22T11:00:05.458119Z","shell.execute_reply":"2024-09-22T11:00:05.468576Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 10))\nfor images, _ in train_ds.take(1):\n    for i in range(9):\n        augmented_images = data_augmentation(images)\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(np.array(augmented_images[0]).astype(\"uint8\"))\n        plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2024-09-22T11:00:09.077698Z","iopub.execute_input":"2024-09-22T11:00:09.078087Z","iopub.status.idle":"2024-09-22T11:00:10.836747Z","shell.execute_reply.started":"2024-09-22T11:00:09.078049Z","shell.execute_reply":"2024-09-22T11:00:10.835818Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ds = train_ds.map(\n    lambda img, label: (data_augmentation(tf.image.resize(img, (224, 224))), label),\n    num_parallel_calls=tf_data.AUTOTUNE,)\ntrain_ds = train_ds.map(\n    lambda img, label: (data_augmentation(img), label),\n    num_parallel_calls=tf_data.AUTOTUNE\n)\ntrain_ds = train_ds.prefetch(tf_data.AUTOTUNE)\nval_ds = val_ds.prefetch(tf_data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T11:00:17.616928Z","iopub.execute_input":"2024-09-22T11:00:17.617695Z","iopub.status.idle":"2024-09-22T11:00:17.886561Z","shell.execute_reply.started":"2024-09-22T11:00:17.617654Z","shell.execute_reply":"2024-09-22T11:00:17.885576Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import keras\nfrom keras import layers\n\ndef skin_cancer_cnn(input_shape, num_classes):\n    inputs = keras.Input(shape=input_shape)\n\n    # Normalization\n    x = layers.Rescaling(1.0 / 255)(inputs)\n\n    # Block 1\n    x = layers.Conv2D(32, 3, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n    x = layers.Dropout(0.3)(x)\n\n    # Block 2\n    x = layers.Conv2D(64, 3, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n    x = layers.Dropout(0.3)(x)\n\n    # Block 3\n    x = layers.Conv2D(128, 3, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n    x = layers.Dropout(0.4)(x)\n\n    # Block 4\n    x = layers.Conv2D(256, 3, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n    x = layers.Dropout(0.4)(x)\n\n    # Flatten and Fully Connected Layers\n    x = layers.Flatten()(x)\n    x = layers.Dense(512)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n    x = layers.Dropout(0.5)(x)\n\n    if num_classes == 2:\n        units = 2\n    else:\n        units = num_classes\n\n    outputs = layers.Dense(units, activation='softmax')(x)\n\n    return keras.Model(inputs, outputs)\n\n# Define the input shape and number of classes\ninput_shape = (224, 224, 3)\nnum_classes = 2  # Assuming binary classification for skin cancer detection\n\n# Create the model\nmodel = skin_cancer_cnn(input_shape=input_shape, num_classes=num_classes)\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T11:00:21.632523Z","iopub.execute_input":"2024-09-22T11:00:21.633231Z","iopub.status.idle":"2024-09-22T11:00:21.809533Z","shell.execute_reply.started":"2024-09-22T11:00:21.63319Z","shell.execute_reply":"2024-09-22T11:00:21.808639Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras import callbacks\nepochs = 5\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4),\n    # Use categorical crossentropy for multi-class problems\n    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n    metrics=[tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\")],\n)\ncallbacks_list = [\n    callbacks.ModelCheckpoint(\"best_model.keras\", monitor=\"val_loss\", save_best_only=True, mode=\"min\"),\n    callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\"),\n]\nhistory = model.fit(\n    train_generator,\n    epochs=epochs,\n    callbacks=callbacks_list,\n    validation_data=validation_generator\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T11:42:20.832808Z","iopub.execute_input":"2024-09-22T11:42:20.833522Z","iopub.status.idle":"2024-09-22T11:52:08.483456Z","shell.execute_reply.started":"2024-09-22T11:42:20.83348Z","shell.execute_reply":"2024-09-22T11:52:08.482635Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data = []\ntest_labels = []\nfor _ in range(len(validation_generator)): # Loop through the entire generator\n  batch_x, batch_y =next(validation_generator)\n  test_data.append(batch_x)\n  test_labels.append(batch_y)\n\n# Concatenate all batches into a single array\ntest_data = np.concatenate(test_data, axis=0)\ntest_labels = np.concatenate(test_labels, axis=0)\n\n# Assuming you have test_data and test_labels ready\nloss, accuracy, f1, recall,precision = evaluate_model(model, test_data, test_labels)\nprint(f'Loss: {loss:.4f}')\nprint(f'Accuracy: {accuracy:.4f}')\nprint(f'F1 Score: {f1:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'Precision: {precision:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-09-22T12:27:12.671594Z","iopub.execute_input":"2024-09-22T12:27:12.672402Z","iopub.status.idle":"2024-09-22T12:27:17.12656Z","shell.execute_reply.started":"2024-09-22T12:27:12.672358Z","shell.execute_reply":"2024-09-22T12:27:17.125555Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Predict the probabilities for the test dataset using the validation_generator\ny_pred_prob = model.predict(validation_generator)\n\n# Convert probabilities to class predictions using argmax\ny_pred = np.argmax(y_pred_prob, axis=-1)\n\n# Get true labels from the generator\ny_true = validation_generator.classes\n\n# Calculate confusion matrix\ncm = confusion_matrix(y_true, y_pred)\n\n# Plot the confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-08T07:03:15.245286Z","iopub.execute_input":"2024-09-08T07:03:15.246324Z","iopub.status.idle":"2024-09-08T07:03:17.93226Z","shell.execute_reply.started":"2024-09-08T07:03:15.246277Z","shell.execute_reply":"2024-09-08T07:03:17.931375Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n\n# Load the ResNet50 model pre-trained on ImageNet, excluding the top fully connected layers\nbase_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n# Add global average pooling layer\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\n\n# Add a fully connected layer with 1024 units and ReLU activation\nx = Dense(1024, activation='relu')(x)\n\n# Add a final fully connected layer with a sigmoid activation for binary classification\npredictions = Dense(2, activation='softmax')(x)  # 1 unit for binary classification\n\n# Create the final model\nmodel1 = Model(inputs=base_model.input, outputs=predictions)\n\n# Optionally, freeze the layers of the base model during training\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# Compile the model with binary crossentropy for binary classification\nmodel1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Display the model summary\nmodel1.summary()","metadata":{"execution":{"iopub.status.busy":"2024-09-22T11:53:33.590879Z","iopub.execute_input":"2024-09-22T11:53:33.591563Z","iopub.status.idle":"2024-09-22T11:53:34.991838Z","shell.execute_reply.started":"2024-09-22T11:53:33.591516Z","shell.execute_reply":"2024-09-22T11:53:34.990946Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras import callbacks\nepochs = 10\nmodel1.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4),\n    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n    metrics=[tf.keras.metrics.BinaryAccuracy(name=\"accuracy\")],\n)\ncallbacks_list = [\n    callbacks.ModelCheckpoint(\"best_model.keras\", monitor=\"val_loss\", save_best_only=True, mode=\"min\"),\n    callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\"),\n]\nhistory = model1.fit(\n    train_generator,\n    epochs=epochs,\n    callbacks=callbacks_list,\n    validation_data=validation_generator\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T11:54:01.5084Z","iopub.execute_input":"2024-09-22T11:54:01.509069Z","iopub.status.idle":"2024-09-22T12:12:59.865234Z","shell.execute_reply.started":"2024-09-22T11:54:01.509028Z","shell.execute_reply":"2024-09-22T12:12:59.864267Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data = []\ntest_labels = []\nfor _ in range(len(validation_generator)): # Loop through the entire generator\n  batch_x, batch_y =next(validation_generator)\n  test_data.append(batch_x)\n  test_labels.append(batch_y)\n\n# Concatenate all batches into a single array\ntest_data = np.concatenate(test_data, axis=0)\ntest_labels = np.concatenate(test_labels, axis=0)\n\n# Assuming you have test_data and test_labels ready\nloss, accuracy, f1, recall,precision = evaluate_model(model1, test_data, test_labels)\nprint(f'Loss: {loss:.4f}')\nprint(f'Accuracy: {accuracy:.4f}')\nprint(f'F1 Score: {f1:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'Precision: {precision:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-09-22T12:26:17.80923Z","iopub.execute_input":"2024-09-22T12:26:17.810013Z","iopub.status.idle":"2024-09-22T12:26:22.280139Z","shell.execute_reply.started":"2024-09-22T12:26:17.809972Z","shell.execute_reply":"2024-09-22T12:26:22.279179Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Predict the probabilities for the test dataset\ny_pred_prob = model1.predict(test_data)\n\n# Convert probabilities to binary predictions (threshold = 0.5)\ny_pred = (y_pred_prob > 0.5).astype(int)\n\n# If the output is from a softmax layer with multiple classes\nif y_pred.shape[-1] > 1:\n    y_pred = np.argmax(y_pred, axis=-1)\n    test_labels = np.argmax(test_labels, axis=-1)\nelse:\n    y_pred = y_pred.flatten()\n\n# Calculate confusion matrix\ncm = confusion_matrix(test_labels, y_pred)\n\n# Plot the confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-08T07:25:15.873673Z","iopub.execute_input":"2024-09-08T07:25:15.874587Z","iopub.status.idle":"2024-09-08T07:25:18.739583Z","shell.execute_reply.started":"2024-09-08T07:25:15.874537Z","shell.execute_reply":"2024-09-08T07:25:18.738685Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras.models import Model\n\n# Load the VGG-19 model with pre-trained weights on ImageNet\n# Set include_top to False to exclude the final fully connected layers\nbase_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n# Freeze the convolutional layers to avoid retraining them\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# Add custom classification layers on top of the base model\nx = Flatten()(base_model.output)  # Flatten the output of the conv layers\nx = Dense(256, activation='relu')(x)  # Add a fully connected layer with 256 neurons\nx = Dense(128, activation='relu')(x)  # Add another fully connected layer\nx = Dense(2, activation='softmax')(x)  # Output layer with 1 neuron and sigmoid activation for binary classification\n\n# Create the new model\nmodel3 = Model(inputs=base_model.input, outputs=x)\n\n# Compile the model\n# Use binary_crossentropy for binary classification\nmodel3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Print a summary of the model architecture\nmodel3.summary()","metadata":{"execution":{"iopub.status.busy":"2024-09-22T12:27:40.096949Z","iopub.execute_input":"2024-09-22T12:27:40.097635Z","iopub.status.idle":"2024-09-22T12:27:40.499058Z","shell.execute_reply.started":"2024-09-22T12:27:40.097596Z","shell.execute_reply":"2024-09-22T12:27:40.498182Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras import callbacks\nepochs = 10\nmodel1.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4),\n    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n    metrics=[tf.keras.metrics.BinaryAccuracy(name=\"accuracy\")],\n)\ncallbacks_list = [\n    callbacks.ModelCheckpoint(\"best_model.keras\", monitor=\"val_loss\", save_best_only=True, mode=\"min\"),\n    callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\"),\n]\nhistory = model3.fit(\n    train_generator,\n    epochs=epochs,\n    callbacks=callbacks_list,\n    validation_data=validation_generator\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T12:27:53.513218Z","iopub.execute_input":"2024-09-22T12:27:53.51361Z","iopub.status.idle":"2024-09-22T12:46:13.80955Z","shell.execute_reply.started":"2024-09-22T12:27:53.51357Z","shell.execute_reply":"2024-09-22T12:46:13.808715Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data = []\ntest_labels = []\nfor _ in range(len(validation_generator)): # Loop through the entire generator\n  batch_x, batch_y =next(validation_generator)\n  test_data.append(batch_x)\n  test_labels.append(batch_y)\n\n# Concatenate all batches into a single array\ntest_data = np.concatenate(test_data, axis=0)\ntest_labels = np.concatenate(test_labels, axis=0)\n\n# Assuming you have test_data and test_labels ready\nloss, accuracy, f1, recall,precision = evaluate_model(model3, test_data, test_labels)\nprint(f'Loss: {loss:.4f}')\nprint(f'Accuracy: {accuracy:.4f}')\nprint(f'F1 Score: {f1:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'Precision: {precision:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-09-22T12:46:46.546714Z","iopub.execute_input":"2024-09-22T12:46:46.547619Z","iopub.status.idle":"2024-09-22T12:46:53.02948Z","shell.execute_reply.started":"2024-09-22T12:46:46.547574Z","shell.execute_reply":"2024-09-22T12:46:53.028546Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Predict the probabilities for the test dataset\ny_pred_prob = model3.predict(test_data)\n\n# Convert probabilities to binary predictions (threshold = 0.5)\ny_pred = (y_pred_prob > 0.5).astype(int)\n\n# If the output is from a softmax layer with multiple classes\nif y_pred.shape[-1] > 1:\n    y_pred = np.argmax(y_pred, axis=-1)\n    test_labels = np.argmax(test_labels, axis=-1)\nelse:\n    y_pred = y_pred.flatten()\n\n# Calculate confusion matrix\ncm = confusion_matrix(test_labels, y_pred)\n\n# Plot the confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-08T07:55:12.511768Z","iopub.execute_input":"2024-09-08T07:55:12.512666Z","iopub.status.idle":"2024-09-08T07:55:16.801228Z","shell.execute_reply.started":"2024-09-08T07:55:12.512623Z","shell.execute_reply":"2024-09-08T07:55:16.800304Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras.models import Model\n\n# Load the VGG-19 model with pre-trained weights on ImageNet\n# Set include_top to False to exclude the final fully connected layers\nbase_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n# Freeze the convolutional layers to avoid retraining them\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# Add custom classification layers on top of the base model\nx = Flatten()(base_model.output)  # Flatten the output of the conv layers\nx = Dense(256, activation='relu')(x)  # Add a fully connected layer with 256 neurons\nx = Dense(128, activation='relu')(x)  # Add another fully connected layer\nx = Dense(2, activation='softmax')(x)  # Output layer with 1 neuron and sigmoid activation for binary classification\n\n# Create the new model\nmodel4 = Model(inputs=base_model.input, outputs=x)\n\n# Compile the model\n# Use binary_crossentropy for binary classification\nmodel4.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Print a summary of the model architecture\nmodel4.summary()","metadata":{"execution":{"iopub.status.busy":"2024-09-22T12:47:08.881385Z","iopub.execute_input":"2024-09-22T12:47:08.882051Z","iopub.status.idle":"2024-09-22T12:47:11.379081Z","shell.execute_reply.started":"2024-09-22T12:47:08.882013Z","shell.execute_reply":"2024-09-22T12:47:11.378085Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras import callbacks\nepochs = 10\n\nmodel4.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\ntrain_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\nval_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255)\ncallbacks_list = [\n    callbacks.ModelCheckpoint(\"best_model.keras\", monitor=\"val_loss\", save_best_only=True, mode=\"min\"),\n    callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\"),\n]\nhistory = model4.fit(\n    train_generator,\n    epochs=epochs,\n    callbacks=callbacks_list,\n    validation_data=validation_generator\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T12:47:16.634006Z","iopub.execute_input":"2024-09-22T12:47:16.634393Z","iopub.status.idle":"2024-09-22T13:05:12.317619Z","shell.execute_reply.started":"2024-09-22T12:47:16.634356Z","shell.execute_reply":"2024-09-22T13:05:12.31679Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Use a loop to get all the batches of data\ntest_data = []\ntest_labels = []\nfor _ in range(len(validation_generator)): # Loop through the entire generator\n  batch_x, batch_y =next(validation_generator)\n  test_data.append(batch_x)\n  test_labels.append(batch_y)\n\n# Concatenate all batches into a single array\ntest_data = np.concatenate(test_data, axis=0)\ntest_labels = np.concatenate(test_labels, axis=0)\n\n# Assuming you have test_data and test_labels ready\nloss, accuracy, f1, recall,precision = evaluate_model(model4, test_data, test_labels)\nprint(f'Loss: {loss:.4f}')\nprint(f'Accuracy: {accuracy:.4f}')\nprint(f'F1 Score: {f1:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'Precision: {precision:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-09-22T13:27:57.608294Z","iopub.execute_input":"2024-09-22T13:27:57.608697Z","iopub.status.idle":"2024-09-22T13:28:02.75692Z","shell.execute_reply.started":"2024-09-22T13:27:57.608658Z","shell.execute_reply":"2024-09-22T13:28:02.75589Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install --upgrade keras","metadata":{"execution":{"iopub.status.busy":"2024-09-21T17:03:41.402653Z","iopub.execute_input":"2024-09-21T17:03:41.403472Z","iopub.status.idle":"2024-09-21T17:03:57.230321Z","shell.execute_reply.started":"2024-09-21T17:03:41.403434Z","shell.execute_reply":"2024-09-21T17:03:57.229107Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n\n# Create a Sequential model\nmodel5 = Sequential()\n\n# Input layer (adjust input_shape as needed)\nmodel5.add(Conv2D(64, (3, 3), activation='relu', input_shape=(224, 224, 3)))\nmodel5.add(MaxPooling2D((2, 2)))\n\n# Residual blocks (adjust the number of blocks and filters as desired)\nmodel5.add(Conv2D(128, (3, 3), activation='relu'))\nmodel5.add(Conv2D(128, (3, 3), activation='relu'))\nmodel5.add(MaxPooling2D((2, 2)))\n\nmodel5.add(Conv2D(256, (3, 3), activation='relu'))\nmodel5.add(Conv2D(256, (3, 3), activation='relu'))\nmodel5.add(MaxPooling2D((2, 2)))\n\nmodel5.add(Conv2D(512, (3, 3), activation='relu'))\nmodel5.add(Conv2D(512, (3, 3), activation='relu'))\nmodel5.add(MaxPooling2D((2, 2)))\n\n# Flatten the feature maps\nmodel5.add(Flatten())\n\n# Fully connected layers (adjust the number of neurons and layers as needed)\nmodel5.add(Dense(256, activation='relu'))\nmodel5.add(Dense(128, activation='relu'))\nmodel5.add(Dense(2, activation='softmax'))\n\n# Compile the model\nmodel5.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-09-22T13:29:13.380857Z","iopub.execute_input":"2024-09-22T13:29:13.381717Z","iopub.status.idle":"2024-09-22T13:29:13.582183Z","shell.execute_reply.started":"2024-09-22T13:29:13.381676Z","shell.execute_reply":"2024-09-22T13:29:13.581314Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras import callbacks\nepochs = 10\n\nmodel5.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\ntrain_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\nval_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255)\n\ncallbacks_list = [\n    callbacks.ModelCheckpoint(\"best_model.keras\", monitor=\"val_loss\", save_best_only=True, mode=\"min\"),\n    callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\"),\n]\nhistory = model5.fit(\n    train_generator,\n    epochs=epochs,\n    callbacks=callbacks_list,\n    validation_data=validation_generator\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T13:29:25.806039Z","iopub.execute_input":"2024-09-22T13:29:25.806764Z","iopub.status.idle":"2024-09-22T13:47:35.904141Z","shell.execute_reply.started":"2024-09-22T13:29:25.806726Z","shell.execute_reply":"2024-09-22T13:47:35.903287Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data = []\ntest_labels = []\nfor _ in range(len(validation_generator)): # Loop through the entire generator\n  batch_x, batch_y =next(validation_generator)\n  test_data.append(batch_x)\n  test_labels.append(batch_y)\n\n# Concatenate all batches into a single array\ntest_data = np.concatenate(test_data, axis=0)\ntest_labels = np.concatenate(test_labels, axis=0)\n\n# Assuming you have test_data and test_labels ready\nloss, accuracy, f1, recall,precision = evaluate_model(model5, test_data, test_labels)\nprint(f'Loss: {loss:.4f}')\nprint(f'Accuracy: {accuracy:.4f}')\nprint(f'F1 Score: {f1:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'Precision: {precision:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-09-22T13:51:48.161535Z","iopub.execute_input":"2024-09-22T13:51:48.162281Z","iopub.status.idle":"2024-09-22T13:51:53.215432Z","shell.execute_reply.started":"2024-09-22T13:51:48.162239Z","shell.execute_reply":"2024-09-22T13:51:53.214524Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import f1_score, recall_score, accuracy_score,precision_score\nfrom keras.metrics import MeanSquaredError\nfrom keras.losses import CategoricalCrossentropy # Changed to CategoricalCrossentropy\nfrom keras.utils import to_categorical\n\n\ndef evaluate_model(model, test_data, test_labels):\n    # Predict the probabilities for the test dataset\n    y_pred_prob = model.predict(test_data)\n\n    # Convert probabilities to class predictions\n    y_pred = np.argmax(y_pred_prob, axis=-1)\n\n    # One-hot encode test_labels\n    #test_labels = to_categorical(test_labels, num_classes=9) # This line is not needed as test_labels are already one-hot encoded.\n\n    # Calculate accuracy\n    accuracy = accuracy_score(np.argmax(test_labels, axis=-1), y_pred)\n\n    # Calculate F1 score\n    # Use 'weighted' average for multiclass data\n    f1 = f1_score(np.argmax(test_labels, axis=-1), y_pred, average='weighted')\n    precision = precision_score(np.argmax(test_labels, axis=-1), y_pred, average='weighted')\n\n    # Calculate recall\n    # Use 'weighted' average for multiclass data\n    recall = recall_score(np.argmax(test_labels, axis=-1), y_pred, average='weighted')\n\n    # Calculate categorical crossentropy loss\n    loss_fn = CategoricalCrossentropy() # Changed to CategoricalCrossentropy\n    loss = loss_fn(test_labels, y_pred_prob).numpy()\n\n    return loss, accuracy, f1, recall,precision","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import keras\nfrom keras import layers\n\ndef skin_cancer_cnn(input_shape, num_classes):\n    inputs = keras.Input(shape=input_shape)\n\n    # Normalization\n    x = layers.Rescaling(1.0 / 255)(inputs)\n\n    # Block 1\n    x = layers.Conv2D(32, 4, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"sigmoid\")(x)\n    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n    x = layers.Dropout(0.3)(x)\n\n    # Block 2\n    x = layers.Conv2D(64, 4, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"sigmoid\")(x)\n    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n    x = layers.Dropout(0.3)(x)\n\n    # Block 3\n    x = layers.Conv2D(128, 4, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"sigmoid\")(x)\n    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n    x = layers.Dropout(0.4)(x)\n\n    # Block 4\n    x = layers.Conv2D(256, 4, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"sigmoid\")(x)\n    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n    x = layers.Dropout(0.4)(x)\n\n    # Flatten and Fully Connected Layers\n    x = layers.Flatten()(x)\n    x = layers.Dense(512)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"sigmoid\")(x)\n    x = layers.Dropout(0.5)(x)\n\n    if num_classes == 2:\n        units = 2\n    else:\n        units = num_classes\n\n    outputs = layers.Dense(units, activation='softmax')(x)\n\n    return keras.Model(inputs, outputs)\n\n# Define the input shape and number of classes\ninput_shape = (224, 224, 3)\nnum_classes = 2  # Assuming binary classification for skin cancer detection\n\n# Create the model\nmodel1 = skin_cancer_cnn(input_shape=input_shape, num_classes=num_classes)\nmodel1.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-22T15:39:07.200047Z","iopub.execute_input":"2024-09-22T15:39:07.200436Z","iopub.status.idle":"2024-09-22T15:39:07.401534Z","shell.execute_reply.started":"2024-09-22T15:39:07.2004Z","shell.execute_reply":"2024-09-22T15:39:07.400655Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras import callbacks\nepochs = 10\n\n# Use categorical_crossentropy for multi-class problems\n\nmodel1.compile(\n    optimizer=tf.keras.optimizers.RMSprop(learning_rate=4e-2),\n    # Use categorical crossentropy for multi-class problems\n    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n    metrics=[tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\")],\n)\ncallbacks_list = [\n    callbacks.ModelCheckpoint(\"best_model.keras\", monitor=\"val_loss\", save_best_only=True, mode=\"min\"),\n    callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\"),\n]\nhistory = model1.fit(\n    train_generator,\n    epochs=epochs,\n    callbacks=callbacks_list,\n    validation_data=validation_generator\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T15:39:17.906082Z","iopub.execute_input":"2024-09-22T15:39:17.906467Z","iopub.status.idle":"2024-09-22T16:01:37.398516Z","shell.execute_reply.started":"2024-09-22T15:39:17.906432Z","shell.execute_reply":"2024-09-22T16:01:37.397723Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data = []\ntest_labels = []\nfor _ in range(len(validation_generator)): # Loop through the entire generator\n  batch_x, batch_y =next(validation_generator)\n  test_data.append(batch_x)\n  test_labels.append(batch_y)\n\n# Concatenate all batches into a single array\ntest_data = np.concatenate(test_data, axis=0)\ntest_labels = np.concatenate(test_labels, axis=0)\n\n# Assuming you have test_data and test_labels ready\nloss, accuracy, f1, recall,precision = evaluate_model(model1, test_data, test_labels)\nprint(f'Loss: {loss:.4f}')\nprint(f'Accuracy: {accuracy:.4f}')\nprint(f'F1 Score: {f1:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'Precision: {precision:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:01:44.143343Z","iopub.execute_input":"2024-09-22T16:01:44.144356Z","iopub.status.idle":"2024-09-22T16:01:49.888942Z","shell.execute_reply.started":"2024-09-22T16:01:44.144311Z","shell.execute_reply":"2024-09-22T16:01:49.887945Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n\n# Load the ResNet50 model pre-trained on ImageNet, excluding the top fully connected layers\nbase_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n# Add global average pooling layer\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\n\n# Add a fully connected layer with 1024 units and ReLU activation\nx = Dense(256, activation='sigmoid')(x)\n\n# Add a final fully connected layer with a sigmoid activation for binary classification\npredictions = Dense(2, activation='softmax')(x)  # 1 unit for binary classification\n\n# Create the final model\nmodel2 = Model(inputs=base_model.input, outputs=predictions)\n\n# Optionally, freeze the layers of the base model during training\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# Compile the model with binary crossentropy for binary classification\nmodel2.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Display the model summary\nmodel2.summary()","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:01:59.936129Z","iopub.execute_input":"2024-09-22T16:01:59.936531Z","iopub.status.idle":"2024-09-22T16:02:04.530498Z","shell.execute_reply.started":"2024-09-22T16:01:59.93649Z","shell.execute_reply":"2024-09-22T16:02:04.529571Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras import callbacks\nepochs = 15\n\nmodel2.compile(\n    optimizer=tf.keras.optimizers.RMSprop(learning_rate=4e-2),\n    # Use categorical crossentropy for multi-class problems\n    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n    metrics=[tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\")],\n)\ncallbacks_list = [\n    callbacks.ModelCheckpoint(\"best_model.keras\", monitor=\"val_loss\", save_best_only=True, mode=\"min\"),\n    callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\"),\n]\nhistory = model2.fit(\n    train_generator,\n    epochs=epochs,\n    callbacks=callbacks_list,\n    validation_data=validation_generator\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T16:39:39.189501Z","iopub.execute_input":"2024-09-22T16:39:39.190426Z","iopub.status.idle":"2024-09-22T16:58:51.059961Z","shell.execute_reply.started":"2024-09-22T16:39:39.190382Z","shell.execute_reply":"2024-09-22T16:58:51.059175Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data = []\ntest_labels = []\nfor _ in range(len(validation_generator)): # Loop through the entire generator\n  batch_x, batch_y =next(validation_generator)\n  test_data.append(batch_x)\n  test_labels.append(batch_y)\n\n# Concatenate all batches into a single array\ntest_data = np.concatenate(test_data, axis=0)\ntest_labels = np.concatenate(test_labels, axis=0)\n\n# Assuming you have test_data and test_labels ready\nloss, accuracy, f1, recall,precision = evaluate_model(model2, test_data, test_labels)\nprint(f'Loss: {loss:.4f}')\nprint(f'Accuracy: {accuracy:.4f}')\nprint(f'F1 Score: {f1:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'Precision: {precision:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-09-22T17:07:22.307666Z","iopub.execute_input":"2024-09-22T17:07:22.308553Z","iopub.status.idle":"2024-09-22T17:07:32.619113Z","shell.execute_reply.started":"2024-09-22T17:07:22.308514Z","shell.execute_reply":"2024-09-22T17:07:32.618219Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras.models import Model\n\n# Load the VGG-19 model with pre-trained weights on ImageNet\n# Set include_top to False to exclude the final fully connected layers\nbase_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n# Freeze the convolutional layers to avoid retraining them\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# Add custom classification layers on top of the base model\nx = Flatten()(base_model.output)  # Flatten the output of the conv layers\nx = Dense(1028, activation='sigmoid')(x)  # Add a fully connected layer with 256 neurons\nx = Dense(128, activation='sigmoid')(x)  # Add another fully connected layer\nx = Dense(2, activation='softmax')(x)  # Output layer with 1 neuron and sigmoid activation for binary classification\n\n# Create the new model\nmodel3 = Model(inputs=base_model.input, outputs=x)\n\n# Print a summary of the model architecture\nmodel3.summary()","metadata":{"execution":{"iopub.status.busy":"2024-09-22T17:07:40.588772Z","iopub.execute_input":"2024-09-22T17:07:40.589613Z","iopub.status.idle":"2024-09-22T17:07:43.878901Z","shell.execute_reply.started":"2024-09-22T17:07:40.589574Z","shell.execute_reply":"2024-09-22T17:07:43.877872Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras import callbacks\nepochs = 15\n\nmodel3.compile(\n    optimizer=tf.keras.optimizers.RMSprop(learning_rate=4e-2),\n    # Use categorical crossentropy for multi-class problems\n    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n    metrics=[tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\")],\n)\ncallbacks_list = [\n    callbacks.ModelCheckpoint(\"best_model.keras\", monitor=\"val_loss\", save_best_only=True, mode=\"min\"),\n    callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\"),\n]\nhistory = model3.fit(\n    train_generator,\n    epochs=epochs,\n    callbacks=callbacks_list,\n    validation_data=validation_generator\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-22T17:07:50.687004Z","iopub.execute_input":"2024-09-22T17:07:50.687739Z","iopub.status.idle":"2024-09-22T17:24:51.883162Z","shell.execute_reply.started":"2024-09-22T17:07:50.687697Z","shell.execute_reply":"2024-09-22T17:24:51.882302Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data = []\ntest_labels = []\nfor _ in range(len(validation_generator)): # Loop through the entire generator\n  batch_x, batch_y =next(validation_generator)\n  test_data.append(batch_x)\n  test_labels.append(batch_y)\n\n# Concatenate all batches into a single array\ntest_data = np.concatenate(test_data, axis=0)\ntest_labels = np.concatenate(test_labels, axis=0)\n\n# Assuming you have test_data and test_labels ready\nloss, accuracy, f1, recall,precision = evaluate_model(model3, test_data, test_labels)\nprint(f'Loss: {loss:.4f}')\nprint(f'Accuracy: {accuracy:.4f}')\nprint(f'F1 Score: {f1:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'Precision: {precision:.4f}')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras.models import Model\n\n# Load the VGG-19 model with pre-trained weights on ImageNet\n# Set include_top to False to exclude the final fully connected layers\nbase_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n# Freeze the convolutional layers to avoid retraining them\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# Add custom classification layers on top of the base model\nx = Flatten()(base_model.output)  # Flatten the output of the conv layers\nx = Dense(1028, activation='sigmoid')(x)  # Add a fully connected layer with 256 neurons\nx = Dense(128, activation='sigmoid')(x)  # Add another fully connected layer\nx = Dense(2, activation='softmax')(x)  # Output layer with 1 neuron and sigmoid activation for binary classification\n\n# Create the new model\nmodel4 = Model(inputs=base_model.input, outputs=x)\n\n# Print a summary of the model architecture\nmodel4.summary()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras import callbacks\nepochs = 15\n\nmodel4.compile(\n    optimizer=tf.keras.optimizers.RMSprop(learning_rate=4e-2),\n    # Use categorical crossentropy for multi-class problems\n    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n    metrics=[tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\")],\n)\ncallbacks_list = [\n    callbacks.ModelCheckpoint(\"best_model.keras\", monitor=\"val_loss\", save_best_only=True, mode=\"min\"),\n    callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\"),\n]\nhistory = model4.fit(\n    train_generator,\n    epochs=epochs,\n    callbacks=callbacks_list,\n    validation_data=validation_generator\n)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data = []\ntest_labels = []\nfor _ in range(len(validation_generator)): # Loop through the entire generator\n  batch_x, batch_y =next(validation_generator)\n  test_data.append(batch_x)\n  test_labels.append(batch_y)\n\n# Concatenate all batches into a single array\ntest_data = np.concatenate(test_data, axis=0)\ntest_labels = np.concatenate(test_labels, axis=0)\n\n# Assuming you have test_data and test_labels ready\nloss, accuracy, f1, recall,precision = evaluate_model(model4, test_data, test_labels)\nprint(f'Loss: {loss:.4f}')\nprint(f'Accuracy: {accuracy:.4f}')\nprint(f'F1 Score: {f1:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'Precision: {precision:.4f}')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n\n# Create a Sequential model\nmodel5 = Sequential()\n\n# Input layer (adjust input_shape as needed)\nmodel5.add(Conv2D(64, (4, 4), activation='sigmoid', input_shape=(224, 224, 3)))\nmodel5.add(MaxPooling2D((3, 3)))\n\n# Residual blocks (adjust the number of blocks and filters as desired)\nmodel5.add(Conv2D(128, (4, 4), activation='sigmoid'))\nmodel5.add(Conv2D(128, (4, 4), activation='sigmoid')\nmodel5.add(MaxPooling2D((3, 3)))\n\nmodel5.add(Conv2D(256, (4, 4), activation='sigmoid'))\nmodel5.add(Conv2D(256, (4, 4), activation='sigmoid')\nmodel5.add(MaxPooling2D((3, 3)))\n\nmodel5.add(Conv2D(512, (4, 4), activation='sigmoid'))\nmodel5.add(Conv2D(512, (4, 4), activation='sigmoid'))\nmodel5.add(MaxPooling2D((3, 3)))\n\n# Flatten the feature maps\nmodel5.add(Flatten())\n\n# Fully connected layers (adjust the number of neurons and layers as needed)\nmodel5.add(Dense(256, activation='sigmoid'))\nmodel5.add(Dense(128, activation='sigmoid'))\nmodel5.add(Dense(2, activation='softmax'))\n\nmodel.summary()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras import callbacks\nepochs = 15\n\nmodel5.compile(\n    optimizer=tf.keras.optimizers.RMSprop(learning_rate=4e-2),\n    # Use categorical crossentropy for multi-class problems\n    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n    metrics=[tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\")],\n)\ncallbacks_list = [\n    callbacks.ModelCheckpoint(\"best_model.keras\", monitor=\"val_loss\", save_best_only=True, mode=\"min\"),\n    callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\"),\n]\nhistory = model5.fit(\n    train_generator,\n    epochs=epochs,\n    callbacks=callbacks_list,\n    validation_data=validation_generator\n)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data = []\ntest_labels = []\nfor _ in range(len(validation_generator)): # Loop through the entire generator\n  batch_x, batch_y =next(validation_generator)\n  test_data.append(batch_x)\n  test_labels.append(batch_y)\n\n# Concatenate all batches into a single array\ntest_data = np.concatenate(test_data, axis=0)\ntest_labels = np.concatenate(test_labels, axis=0)\n\n# Assuming you have test_data and test_labels ready\nloss, accuracy, f1, recall,precision = evaluate_model(model5, test_data, test_labels)\nprint(f'Loss: {loss:.4f}')\nprint(f'Accuracy: {accuracy:.4f}')\nprint(f'F1 Score: {f1:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'Precision: {precision:.4f}')","metadata":{},"outputs":[],"execution_count":null}]}